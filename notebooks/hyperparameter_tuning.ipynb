{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c234c556-bb48-451b-98de-4952491000f4",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.backends.cudnn as cudnn\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import mlflow\n",
    "import time\n",
    "import gc\n",
    "import multiprocessing as mp\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from forecaster.scalers import *\n",
    "from forecaster.preprocessing import TimeseriesDataSet, Granularity\n",
    "from forecaster.models import *\n",
    "from forecaster.training import EarlyStopper, ModelTrainer, TimeseriesForecaster\n",
    "from forecaster.evaluation import evaluate_series, top_n_by_metric, get_full_results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment setup\n",
    "os.makedirs(f\"../logs/mlruns/.trash\", exist_ok=True)\n",
    "torch.cuda.empty_cache()\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "cudnn.benchmark = True\n",
    "mp.set_start_method(\"spawn\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeff10e8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Data set configuration\n",
    "PATH = \"../data/train_data.csv\" \n",
    "GRANULARITY = Granularity.HOURLY\n",
    "N_SERIES = 1 # Number of parallel time series in the dataset\n",
    "\n",
    "# Logging configuration\n",
    "USE_MLFLOW = True\n",
    "SAVE_MODEL = False\n",
    "\n",
    "# Untuned Parameters\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 100\n",
    "OUTPUT_SIZE = 24\n",
    "SCALER = StandardScaler\n",
    "TRAIN_VALIDATION_SPLIT = 0.7\n",
    "USE_TIME_COVARIATES = True\n",
    "LR = 0.001\n",
    "DROPOUT = 0.1\n",
    "INPUT_SIZE = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d22933e-acca-449b-b428-744d6aadcf46",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "pandas_df = pd.read_csv(PATH)\n",
    "pandas_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a459f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pandas_df.iloc[:, :N_SERIES + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1664cf0-89e1-4539-a4a3-d62edf6e32f3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "for i in range(min(N_SERIES, 10)):  # Plot only first 10 series\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pandas_df[\"deviceTimestamp\"],\n",
    "        y=pandas_df[f\"value_{i+1}\"],\n",
    "        mode=\"lines\",\n",
    "        name=f\"Value_{i+1}\"\n",
    "    ))\n",
    "fig.update_layout(\n",
    "    title=\"All Series over Time\",\n",
    "    width=1200,\n",
    "    height=400,\n",
    "    xaxis_title=\"deviceTimestamp\",\n",
    "    yaxis_title=\"Value\"\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065dfc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_MAP = {\n",
    "    \"LSTMAttention\": LSTMAttention, \"LSTM\": LSTM, \"GRUAttention\": GRUAttention, \"GRU\": GRU\n",
    "}\n",
    "SCALER_MAP = {\n",
    "    \"StandardScaler\": StandardScaler, \"MinMax\": MinMaxScaler,\n",
    "    \"LogStandardScaler\": LogStandardScaler\n",
    "}\n",
    "LOSS_MAP = {\n",
    "    \"MSELoss\": nn.MSELoss(), \"L1Loss\": nn.L1Loss(), \"HuberLoss\": nn.HuberLoss()\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    model_name = \"GRU\" #trial.suggest_categorical(\"model_type\", list(MODEL_MAP.keys()))\n",
    "    scaler_name = trial.suggest_categorical(\"scaler\", list(SCALER_MAP.keys()))\n",
    "    loss_name = trial.suggest_categorical(\"loss_fn\", list(LOSS_MAP.keys()))\n",
    "    \n",
    "    time_covariates = USE_TIME_COVARIATES\n",
    "    learning_rate = LR\n",
    "    hidden_dim = trial.suggest_categorical(\"hidden_dim\", [16, 32, 64]) \n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 8, step=2)\n",
    "    input_size = trial.suggest_categorical(\"input_size\", [24, 48, 72])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5, step=0.1)\n",
    "\n",
    "    ModelClass = MODEL_MAP[model_name]\n",
    "    scaler_class = SCALER_MAP[scaler_name]\n",
    "    loss_fn = LOSS_MAP[loss_name]\n",
    "    \n",
    "    ds = TimeseriesDataSet(\n",
    "        pandas_df, GRANULARITY, time_covariates,\n",
    "        \"deviceTimestamp\", TRAIN_VALIDATION_SPLIT,\n",
    "        scaler_class, input_size, OUTPUT_SIZE\n",
    "    )\n",
    "    N_COV = ds.get_time_feature_count()\n",
    "\n",
    "    model = ModelClass(\n",
    "        input_dim=1 + N_COV,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        output_dim=OUTPUT_SIZE,\n",
    "        dropout=dropout\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=N_EPOCHS)\n",
    "    stopper = EarlyStopper(patience=20, min_delta=1e-4)\n",
    "\n",
    "    def optuna_callback(epoch, metrics):\n",
    "        val_rmse = metrics['val_rmse']\n",
    "        val_mae = metrics['val_mae']\n",
    "        train_rmse = metrics['train_rmse']\n",
    "        train_mae = metrics['train_mae']\n",
    "        learning_rate = metrics['learning_rate']\n",
    "        epoch_time = metrics['epoch_time']\n",
    "        \n",
    "        print(f\"Epoch {epoch}: Train RMSE {train_rmse:.4f} | Train MAE {train_mae:.4f} | Val RMSE {val_rmse:.4f} | Val MAE {val_mae:.4f} | LR {learning_rate:.6f} | Epoch time {epoch_time:.2f}s\")\n",
    "\n",
    "    tr = ModelTrainer(\n",
    "        model, DEVICE, opt, loss_fn, scheduler,\n",
    "        ds.get_train_dataloader(BATCH_SIZE, True),\n",
    "        ds.get_validation_dataloader(BATCH_SIZE),\n",
    "        early_stopper=stopper\n",
    "    )\n",
    "\n",
    "    # Fit the model\n",
    "    tr.fit(N_EPOCHS, on_epoch_end=optuna_callback)\n",
    "\n",
    "    forecaster = TimeseriesForecaster(model, DEVICE, input_size, OUTPUT_SIZE)\n",
    "    start_time = time.time()\n",
    "    predictions_dict = forecaster.predict_all_series(ds, BATCH_SIZE)\n",
    "    print(f\"Prediction Time: {time.time() - start_time:.2f} seconds\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    unscaled_summary = evaluate_series(\n",
    "        dataset=ds,\n",
    "        preds=predictions_dict,\n",
    "        input_size=INPUT_SIZE,\n",
    "        output_size=OUTPUT_SIZE\n",
    "    )\n",
    "    print(f\"Evaluation Time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "    val_mae = np.mean([m['val_mae'] for m in unscaled_summary.values()])\n",
    "\n",
    "    del ds, model, opt, tr, scheduler, forecaster, predictions_dict, unscaled_summary\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return val_mae\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "print(\"Starting Optimization...\")\n",
    "study.optimize(objective, n_trials=20) \n",
    "\n",
    "print(\"Best MAE:\", study.best_value)\n",
    "print(\"Best Params:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12059367",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(f\"Retraining best model with params: {study.best_params}\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "best_params = study.best_params\n",
    "ModelClass = GRU #MODEL_MAP[best_params[\"model_type\"]]\n",
    "scaler_class = SCALER_MAP[best_params[\"scaler\"]]\n",
    "loss_fn = LOSS_MAP[best_params[\"loss_fn\"]]\n",
    "\n",
    "dataset = TimeseriesDataSet(\n",
    "    pandas_df, GRANULARITY, USE_TIME_COVARIATES,\n",
    "    \"deviceTimestamp\", TRAIN_VALIDATION_SPLIT,\n",
    "    scaler_class, best_params[\"input_size\"], OUTPUT_SIZE\n",
    ")\n",
    "N_COV = dataset.get_time_feature_count()\n",
    "\n",
    "model = ModelClass(\n",
    "    input_dim=1 + N_COV,\n",
    "    hidden_dim=best_params[\"hidden_dim\"],\n",
    "    num_layers=best_params[\"num_layers\"],\n",
    "    output_dim=OUTPUT_SIZE,\n",
    "    dropout=DROPOUT\n",
    ").to(DEVICE)\n",
    "\n",
    "opt = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, T_max=N_EPOCHS)\n",
    "stopper = EarlyStopper(patience=10, min_delta=1e-4)\n",
    "\n",
    "trainer = ModelTrainer(\n",
    "    model, DEVICE, opt, loss_fn, scheduler,\n",
    "    dataset.get_train_dataloader(BATCH_SIZE, True),\n",
    "    dataset.get_validation_dataloader(BATCH_SIZE),\n",
    "    early_stopper=stopper\n",
    ")\n",
    "\n",
    "history = trainer.fit(N_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9830c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../logs/model_architecture.txt\", \"w\") as f:\n",
    "    f.write(str(model))\n",
    "torch.save(model.state_dict(), \"../logs/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c941d49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention heatmap over validation set: rows=lag, cols=windows, colors=attention weights\n",
    "model.eval()\n",
    "key = pandas_df.columns[1]\n",
    "\n",
    "if isinstance(model, (GRUAttention, LSTMAttention)):\n",
    "    with torch.no_grad():\n",
    "        val_dataloader = dataset.get_single_series_dataloader(key, \"validation\", BATCH_SIZE, shuffle=False)\n",
    "        seq_len = dataset.input_size\n",
    "        A_list = []\n",
    "        for X_batch, y_batch in val_dataloader:\n",
    "            inputs = X_batch.to(DEVICE)\n",
    "            _, attn_weights = model(inputs, return_attention=True)\n",
    "            A_list.append(attn_weights.cpu().numpy())\n",
    "        A = np.vstack(A_list)  # shape (n_windows, seq_len)\n",
    "        # timestamps for validation range\n",
    "        val_timestamps = dataset.get_resampled_data()[dataset.timestamp_column].iloc[dataset.n_train:].reset_index(drop=True)\n",
    "        # map attention windows into heatmap matrix M (rows = window index, cols = validation timestamps)\n",
    "        n_windows = 0 if A.size == 0 else A.shape[0]\n",
    "        M = np.full((n_windows, len(val_timestamps)), np.nan)\n",
    "        for w in range(n_windows):\n",
    "            end_col = min(w + seq_len, len(val_timestamps))\n",
    "            M[w, w:end_col] = A[w, : end_col - w]\n",
    "\n",
    "    # Limit to last week of validation timestamps\n",
    "    if GRANULARITY == Granularity.HOURLY:\n",
    "        last_week_mask = val_timestamps >= (val_timestamps.max() - pd.Timedelta(days=7))\n",
    "    elif GRANULARITY == Granularity.DAILY:\n",
    "        last_week_mask = val_timestamps >= (val_timestamps.max() - pd.Timedelta(days=7))\n",
    "    elif GRANULARITY == Granularity.MONTHLY:\n",
    "        last_week_mask = val_timestamps >= (val_timestamps.max() - pd.DateOffset(months=1))\n",
    "    else:\n",
    "        last_week_mask = np.ones(len(val_timestamps), dtype=bool)\n",
    "    last_week_mask = last_week_mask.values.astype(bool)\n",
    "    val_timestamps = val_timestamps[last_week_mask].reset_index(drop=True)\n",
    "    M = M[:, last_week_mask]\n",
    "    valid_row_mask = ~np.all(np.isnan(M), axis=1)\n",
    "    M = M[valid_row_mask, :]\n",
    "\n",
    "    # get values of series for the last week\n",
    "    series_values = dataset.get_scaled_data(key)[\"validation\"]\n",
    "    series_values = series_values[last_week_mask].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fc96f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(model, (GRUAttention, LSTMAttention)):\n",
    "    fig_heat = go.Figure()\n",
    "\n",
    "    # Heatmap\n",
    "    fig_heat.add_trace(go.Heatmap(\n",
    "        z=M,\n",
    "        colorscale='Viridis',\n",
    "        colorbar=dict(title='Attention'),\n",
    "        x=val_timestamps,\n",
    "        y=list(range(M.shape[0])),\n",
    "        name='attention'\n",
    "    ))\n",
    "\n",
    "    # Series line on a secondary y-axis so it uses its own value scale\n",
    "    fig_heat.add_trace(go.Scatter(\n",
    "        x=val_timestamps,\n",
    "        y=series_values,\n",
    "        mode='lines',\n",
    "        name=f'{key} values',\n",
    "        line=dict(color='black', width=2),\n",
    "        yaxis='y2'\n",
    "    ))\n",
    "\n",
    "    # Layout: add yaxis2 that overlays the heatmap y-axis\n",
    "    fig_heat.update_layout(\n",
    "        title='Attention heatmap for value_35 (timestamps on x-axis, rows=rolling windows)',\n",
    "        xaxis=dict(title='Timestamp (validation range)'),\n",
    "        yaxis=dict(title='Validation window index (rolling)'),\n",
    "        yaxis2=dict(\n",
    "            title=f'Scaled values for {key}',\n",
    "            overlaying='y',\n",
    "            side='right'\n",
    "        ),\n",
    "        width=1200,\n",
    "        height=600,\n",
    "        legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1)\n",
    "    )\n",
    "\n",
    "    fig_heat.show()\n",
    "    fig_heat.write_html(\"../logs/attention_heatmap.html\", include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2458d1f5-166e-4981-87cc-25f1729b82e0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Train-Validation RMSE Plot (linear + log scale)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(history[\"train_rmse\"], marker='o', label='Train RMSE')\n",
    "axes[0].plot(history[\"val_rmse\"], marker='x', label='Val RMSE')\n",
    "axes[0].set_title('RMSE over Epochs (linear)')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('RMSE')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, which='both', alpha=0.3)\n",
    "\n",
    "axes[1].plot(history[\"train_rmse\"], marker='o', label='Train RMSE')\n",
    "axes[1].plot(history[\"val_rmse\"], marker='x', label='Val RMSE')\n",
    "axes[1].set_yscale('log')\n",
    "axes[1].set_title('RMSE over Epochs (log scale)')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('RMSE (log)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, which='both', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"../logs/rmse.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92cbf3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 5))\n",
    "plt.plot(history[\"epoch_time\"], marker='o')\n",
    "plt.axhline(y=np.mean(history[\"epoch_time\"]), color='r', linestyle='--', label='Avg Epoch Time')\n",
    "plt.legend()\n",
    "plt.title('Epoch Times')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Time (s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"../logs/epoch_times.png\", bbox_inches='tight', dpi=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a40ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecaster = TimeseriesForecaster(model, DEVICE, INPUT_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "print(\"Generating predictions for all series...\")\n",
    "start_time = time.time()\n",
    "predictions_dict = forecaster.predict_all_series(dataset, BATCH_SIZE)\n",
    "print(f\"Predictions generated in {time.time() - start_time:.2f}s\")\n",
    "\n",
    "summary = evaluate_series(\n",
    "    dataset=dataset,\n",
    "    preds=predictions_dict,\n",
    "    input_size=INPUT_SIZE,\n",
    "    output_size=OUTPUT_SIZE\n",
    ")\n",
    "\n",
    "top_5 = top_n_by_metric(summary, n=5, metric='val_mae', reverse=False)\n",
    "bottom_5 = top_n_by_metric(summary, n=5, metric='val_mae', reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b2bda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.mean([m['val_mae'] for m in summary.values()]))\n",
    "print(np.mean([m['val_rmse'] for m in summary.values()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b7f944-0131-4616-b06f-146cbc5480fe",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "print(\"Top 5 series by val_mae:\")\n",
    "for k in top_5:\n",
    "    val = summary[k].get('val_mae', np.nan)\n",
    "    print(k, f\"{float(val):.2f}\")\n",
    "print(\"Bottom 5 series by val_mae:\")\n",
    "for k in bottom_5:\n",
    "    val = summary[k].get('val_mae', np.nan)\n",
    "    print(k, f\"{float(val):.2f}\")\n",
    "\n",
    "full_results_dict = get_full_results_dict(dataset, predictions_dict, INPUT_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "# Plot MAE for all series (validation set), sorted by MAE\n",
    "mae_vals = [float(summary[k]['val_mae']) for k in summary]\n",
    "series_names = list(summary.keys())\n",
    "\n",
    "# Sort by MAE\n",
    "sorted_indices = np.argsort(mae_vals)\n",
    "sorted_mae = [mae_vals[i] for i in sorted_indices]\n",
    "sorted_series = [series_names[i] for i in sorted_indices]\n",
    "\n",
    "fig = px.bar(\n",
    "    x=sorted_series,\n",
    "    y=sorted_mae,\n",
    "    labels={\"x\": \"Series\", \"y\": \"Validation MAE\"},\n",
    "    title=\"Validation MAE for All Series (Sorted)\",\n",
    "    width=1200,\n",
    "    height=400\n",
    ")\n",
    "fig.update_layout(xaxis_tickangle=90)\n",
    "fig.write_html(\"../logs/all_series_mae_sorted.html\", include_plotlyjs='cdn')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2663a7c0-7e74-454e-952a-3b07a9878a03",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Unscaled predictions plot\n",
    "top_flop = {**top_5, **bottom_5}\n",
    "\n",
    "for series_key in top_flop:\n",
    "    full_results_df = full_results_dict[series_key]\n",
    "    \n",
    "    train_mask = ~pd.isna(full_results_df['train_predicted'])\n",
    "    validation_mask = ~pd.isna(full_results_df['validation_predicted'])\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=full_results_df['timestamp'], \n",
    "        y=full_results_df['actual'], \n",
    "        mode='lines', \n",
    "        name='Actual consumption', \n",
    "        line=dict(color='blue', width=1)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=full_results_df.loc[train_mask, 'timestamp'], \n",
    "        y=full_results_df.loc[train_mask, 'train_predicted'], \n",
    "        mode='lines', \n",
    "        name='Training predictions', \n",
    "        line=dict(color='orange', width=1)\n",
    "    ))\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=full_results_df.loc[validation_mask, 'timestamp'], \n",
    "        y=full_results_df.loc[validation_mask, 'validation_predicted'], \n",
    "        mode='lines', \n",
    "        name='Validation predictions', \n",
    "        line=dict(color='red', width=2)\n",
    "    ))\n",
    "    fig.update_layout(\n",
    "        title=f'LSTM Energy Consumption Forecast - Series {series_key}',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Energy Consumption',\n",
    "        width=1200, \n",
    "        height=600,\n",
    "        hovermode='x unified'\n",
    "    )\n",
    "    fig.write_html(f\"../logs/train_validation_truth_{series_key}.html\", include_plotlyjs='cdn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5d9627-2215-4613-9f6e-7a638fb66897",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Results DataFrame for analysis\n",
    "results_dict = {}\n",
    "\n",
    "for series_key in top_flop:\n",
    "    df = full_results_dict[series_key]\n",
    "    validation_mask = ~pd.isna(df['validation_predicted'])\n",
    "    \n",
    "    validation_indices = df.index[validation_mask]\n",
    "    validation_start = validation_indices.min()\n",
    "    validation_end = validation_indices.max() + 1\n",
    "    \n",
    "    # Create results dataframe for each series\n",
    "    results_df = pd.DataFrame({\n",
    "        'timestamp': dataset.get_resampled_data()['deviceTimestamp'].iloc[validation_start:validation_end].reset_index(drop=True),\n",
    "        'actual': df.loc[validation_start:validation_end-1, 'actual'].values,\n",
    "        'predicted': df.loc[validation_start:validation_end-1, 'validation_predicted'].values\n",
    "    })\n",
    "    \n",
    "    if GRANULARITY == Granularity.HOURLY:\n",
    "        results_df['hour_of_day'] = results_df['timestamp'].dt.hour\n",
    "        results_df['day_of_week'] = results_df['timestamp'].dt.day_name()\n",
    "        results_df['month_of_year'] = results_df['timestamp'].dt.month\n",
    "    elif GRANULARITY == Granularity.DAILY:\n",
    "        results_df['day_of_week'] = results_df['timestamp'].dt.day_name()\n",
    "        results_df['month_of_year'] = results_df['timestamp'].dt.month\n",
    "    elif GRANULARITY == Granularity.MONTHLY:\n",
    "        results_df['month_of_year'] = results_df['timestamp'].dt.month\n",
    "    \n",
    "    results_dict[series_key] = results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3674c21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\n",
    "    (\"hour_of_day\", list(range(24)), \"Hour of Day\"),\n",
    "    (\"day_of_week\", ['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'], \"Day of Week\"),\n",
    "    (\"month_of_year\", list(range(1, 13)), \"Month of Year\")\n",
    "]\n",
    "\n",
    "for col_name, labels, xlab in metrics:\n",
    "    if col_name not in next(iter(results_dict.values())).columns:\n",
    "        continue\n",
    "\n",
    "    n = len(top_flop)\n",
    "    cols = min(5, n)\n",
    "    rows = 2 if n > 5 else 1\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(4*cols, 3*rows), squeeze=False)\n",
    "\n",
    "    for idx, series_key in enumerate(top_flop):\n",
    "        ax = axes[idx//cols][idx%cols]\n",
    "        df = results_dict[series_key]\n",
    "\n",
    "        vals = []\n",
    "        for lab in labels:\n",
    "            data = df[df[col_name] == lab]\n",
    "            if not data.empty:\n",
    "                rmse = np.sqrt(np.mean((data[\"actual\"] - data[\"predicted\"])**2))\n",
    "            else:\n",
    "                rmse = np.nan\n",
    "            vals.append(rmse)\n",
    "        ax.bar(labels, vals, alpha=0.7)\n",
    "        ax.set_title(f\"Series {series_key}\")\n",
    "        ax.set_xlabel(xlab)\n",
    "        ax.set_ylabel(\"RMSE\")\n",
    "        ax.tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(f\"../logs/rmse_by_{col_name}.png\", dpi=150)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c5a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_naive_predictions_dict(dataset, input_size, output_size):\n",
    "    naive_preds = {}\n",
    "    for key in dataset.split_unscaled_dict:\n",
    "        naive_preds[key] = {}\n",
    "        \n",
    "        for split in ['train', 'validation']:\n",
    "            data = dataset.get_unscaled_data(key)[split]\n",
    "            preds = []\n",
    "            valid_split_points = range(input_size, len(data) + 1, output_size)\n",
    "            \n",
    "            for i in valid_split_points:\n",
    "                if i - output_size >= 0:\n",
    "                    pred_window = data[i - output_size : i]\n",
    "                    if len(pred_window) == output_size:\n",
    "                        preds.append(pred_window)\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            if preds:\n",
    "                naive_preds[key][split] = np.concatenate(preds).flatten()\n",
    "            else:\n",
    "                naive_preds[key][split] = np.array([])\n",
    "                \n",
    "    return naive_preds\n",
    "\n",
    "naive_predictions_dict = get_naive_predictions_dict(dataset, INPUT_SIZE, OUTPUT_SIZE)\n",
    "\n",
    "naive_summary = evaluate_series(\n",
    "    dataset=dataset, \n",
    "    preds=naive_predictions_dict, \n",
    "    input_size=INPUT_SIZE, \n",
    "    output_size=OUTPUT_SIZE\n",
    ")\n",
    "\n",
    "metric_comparison = {}\n",
    "model_vs_naive = []\n",
    "\n",
    "for series_key in predictions_dict:\n",
    "    if series_key not in naive_summary:\n",
    "        continue\n",
    "        \n",
    "    model_metrics = summary[series_key]\n",
    "    naive_metrics = naive_summary[series_key]\n",
    "\n",
    "    metric_comparison[series_key] = {\n",
    "        \"naive_rmse\": float(naive_metrics[\"val_rmse\"]),\n",
    "        \"naive_mae\": float(naive_metrics[\"val_mae\"]),\n",
    "        \"model_rmse\": float(model_metrics[\"val_rmse\"]),\n",
    "        \"model_mae\": float(model_metrics[\"val_mae\"])\n",
    "    }\n",
    "\n",
    "    model_vs_naive.append({\n",
    "        \"series\": series_key,\n",
    "        \"naive_rmse\": naive_metrics[\"val_rmse\"],\n",
    "        \"model_rmse\": model_metrics[\"val_rmse\"],\n",
    "        \"improved\": model_metrics[\"val_rmse\"] < naive_metrics[\"val_rmse\"]\n",
    "    })\n",
    "\n",
    "naive_df = pd.DataFrame.from_dict(metric_comparison, orient=\"index\")\n",
    "naive_df = naive_df.sort_values(\"naive_rmse\")\n",
    "\n",
    "print(\"Overall averages (Validation Set):\")\n",
    "print(f\" Naive RMSE: {naive_df['naive_rmse'].mean():.4f}, Model RMSE: {naive_df['model_rmse'].mean():.4f}\")\n",
    "print(f\" Naive MAE:  {naive_df['naive_mae'].mean():.4f}, Model MAE:  {naive_df['model_mae'].mean():.4f}\")\n",
    "\n",
    "val_mase = naive_df['model_mae'].mean() / naive_df['naive_mae'].mean()\n",
    "print(f\"Model MASE: {val_mase:.2f} (Value < 1.0 indicates model is better than naive)\")\n",
    "\n",
    "better_count = (naive_df['model_mae'] < naive_df['naive_mae']).sum()\n",
    "print(f\"Model improved over Naive on {better_count} / {len(naive_df)} series.\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,6))\n",
    "ax.scatter(naive_df[\"naive_mae\"], naive_df[\"model_mae\"], alpha=0.6)\n",
    "ax.plot([naive_df[\"naive_mae\"].min(), naive_df[\"naive_mae\"].max()],\n",
    "        [naive_df[\"naive_mae\"].min(), naive_df[\"naive_mae\"].max()], 'r--', label=\"y=x (Parity)\")\n",
    "x_vals = np.array([naive_df[\"naive_mae\"].min(), naive_df[\"naive_mae\"].max()])\n",
    "ax.plot(x_vals, val_mase * x_vals, 'g--', label=f\"Avg Improvement (slope={val_mase:.2f})\")\n",
    "\n",
    "ax.set_xlabel(\"Naive MAE\")\n",
    "ax.set_ylabel(\"Model MAE\")\n",
    "ax.set_title(f\"Model vs Naive (Repeat Last {OUTPUT_SIZE})\")\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7299893",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "# Logging with MLFlow\n",
    "if USE_MLFLOW:\n",
    "    mlflow.set_tracking_uri(\"file:../logs/mlruns\")\n",
    "    try:\n",
    "        mlflow.create_experiment(\"Energy_Consumption_Forecast\")\n",
    "    except:\n",
    "        mlflow.set_experiment(\"Energy_Consumption_Forecast\")\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        # Model Architecture\n",
    "        if SAVE_MODEL:\n",
    "            mlflow.pytorch.log_model(model, artifact_path=\"LSTM forecaster\")\n",
    "        \n",
    "        mlflow.log_artifact(\"../logs/model_architecture.txt\")\n",
    "        mlflow.log_param(\"granularity\", GRANULARITY.value)\n",
    "        mlflow.log_param(\"input_size\", INPUT_SIZE)\n",
    "        mlflow.log_param(\"output_size\", OUTPUT_SIZE)\n",
    "        mlflow.log_param(\"learning_rate\", LR)\n",
    "        mlflow.log_param(\"loss_function\", best_params[\"loss_fn\"])\n",
    "        mlflow.log_param(\"scaler\", best_params[\"scaler\"])\n",
    "        mlflow.log_param(\"batch_size\", BATCH_SIZE)\n",
    "        mlflow.log_param(\"n_epochs\", N_EPOCHS)\n",
    "        mlflow.log_param(\"model_type\", \"LSTMAttention\") #best_params[\"model_type\"])\n",
    "        mlflow.log_param(\"N_SERIES\", N_SERIES)\n",
    "        mlflow.log_param(\"USE_TIME_COVARIATES\", USE_TIME_COVARIATES)\n",
    "\n",
    "        # Model training results\n",
    "        mlflow.log_metric(\"train_rmse\", history[\"train_rmse\"][-1])\n",
    "        mlflow.log_metric(\"train_mae\", history[\"train_mae\"][-1])\n",
    "        mlflow.log_metric(\"train_mae_avg\", np.mean([m['train_mae'] for m in summary.values() if not np.isnan(m['train_mae'])]))\n",
    "        mlflow.log_metric(\"val_mase\", val_mase)\n",
    "        v1 = summary.get(\"value_1\", {})\n",
    "        v1_train_mae = v1.get(\"train_mae\", np.nan)\n",
    "        v1_val_mae = v1.get(\"val_mae\", np.nan)\n",
    "        if not np.isnan(v1_train_mae):\n",
    "            mlflow.log_metric(\"value_1_train_mae\", float(v1_train_mae))\n",
    "        if not np.isnan(v1_val_mae):\n",
    "            mlflow.log_metric(\"value_1_val_mae\", float(v1_val_mae))\n",
    "        mlflow.log_metric(\"val_rmse\", history[\"val_rmse\"][-1])\n",
    "        mlflow.log_metric(\"val_mae\", history[\"val_mae\"][-1])\n",
    "        mlflow.log_metric(\"val_mae_avg\", np.mean([m['val_mae'] for m in summary.values() if not np.isnan(m['val_mae'])]))\n",
    "        mlflow.log_metric(\"total_training_time_sec\", sum(history[\"epoch_time\"]))\n",
    "        mlflow.log_artifact(\"../logs/rmse.png\")\n",
    "        mlflow.log_artifact(\"../logs/epoch_times.png\")\n",
    "        mlflow.log_artifact(\"../logs/attention_heatmap.html\")\n",
    "\n",
    "        # Log all series prediction results\n",
    "        if 'hour_of_day' in next(iter(results_dict.values())).columns:\n",
    "            mlflow.log_artifact(f\"../logs/rmse_by_hour_of_day.png\")\n",
    "        if 'day_of_week' in next(iter(results_dict.values())).columns:\n",
    "            mlflow.log_artifact(f\"../logs/rmse_by_day_of_week.png\")\n",
    "        if 'month_of_year' in next(iter(results_dict.values())).columns:\n",
    "            mlflow.log_artifact(f\"../logs/rmse_by_month_of_year.png\")\n",
    "        \n",
    "        for series_key in top_flop:\n",
    "            mlflow.log_artifact(f\"../logs/train_validation_truth_{series_key}.html\")\n",
    "\n",
    "        mlflow.log_artifact(\"../logs/all_series_mae_sorted.html\")\n",
    "        mlflow.log_artifact(\"../logs/all_series_mape_sorted.html\")\n",
    "        mlflow.log_artifact(\"../logs/naive_vs_model_mae.png\")"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "known_lakehouses": []
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
